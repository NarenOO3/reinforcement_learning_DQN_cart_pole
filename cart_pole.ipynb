{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051dd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a virtual display to render the Lunar Lander environment.\n",
    "Display(visible=0, size=(840, 480)).start()\n",
    "\n",
    "# Set the random seed for TensorFlow\n",
    "tf.random.set_seed(utils.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0935bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEMORY_SIZE = 100_000     # size of memory buffer\n",
    "GAMMA = 0.995             # discount factor\n",
    "ALPHA = 1e-3              # learning rate  \n",
    "NUM_STEPS_FOR_UPDATE = 4  # perform a learning update every C time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\", render_mode=\"rgb_array\", sutton_barto_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbabbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_state = np.array([0.25, 0.0, 0.0, 0.0])  # x=0.5, rest = 0\n",
    "env.unwrapped.state = fixed_state\n",
    "PIL.Image.fromarray(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ecdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "print('State Shape:', state_size)\n",
    "print('Number of actions:', num_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f499a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environment and get the initial state.\n",
    "current_state = env.reset()\n",
    "print(current_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b37ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an action\n",
    "action = 0\n",
    "\n",
    "# Run a single time step of the environment's dynamics with the given action.\n",
    "next_state, reward, terminated_vals, _ , _= env.step(action)\n",
    "\n",
    "print (next_state[0], reward, terminated_vals)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ad1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Q-Network\n",
    "q_network = Sequential([ \n",
    "    Input(shape=state_size),                      \n",
    "    Dense(units=32, activation='relu'),            \n",
    "    Dense(units=32, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "    ])\n",
    "     \n",
    "     \n",
    "# Create the target Q^-Network\n",
    "target_q_network = Sequential([\n",
    "    Input(shape=state_size),                      \n",
    "    Dense(units=32, activation='relu'),            \n",
    "    Dense(units=32, activation='relu'),            \n",
    "    Dense(units=num_actions, activation='linear'),\n",
    "    ])\n",
    "    \n",
    "optimizer = Adam(learning_rate=ALPHA)             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468eb2db",
   "metadata": {},
   "source": [
    "### To Train DQN and DDQN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229cf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, terminated_vals = experiences\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ### For DQN ###\n",
    "    # Compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_network(next_states), axis=-1)\n",
    "    y_targets = rewards + (1-terminated_vals)*(gamma*max_qsa)\n",
    "    \n",
    "    \n",
    "    # ### For Double DQN ###\n",
    "    # # Q-values from target network for next_states\n",
    "    # q_values_target = target_q_network(next_states)  # shape [batch_size, num_actions]\n",
    "    # q_values = q_network(next_states)\n",
    "    # # Greedy actions chosen by q_network\n",
    "    # next_actions = tf.argmax(q_values, axis=1,output_type=tf.int32)  # shape [batch_size]\n",
    "    # # Gather the Q-value for each chosen action\n",
    "    # batch_indices = tf.range(tf.shape(next_actions)[0], dtype=tf.int32)\n",
    "    # indices = tf.stack([batch_indices, next_actions], axis=1)\n",
    "    # q_next_target = tf.gather_nd(q_values_target, indices)  # shape [batch_size]\n",
    "    # y_targets = rewards + (1-terminated_vals)*(gamma*q_next_target)\n",
    "    \n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a). \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the loss\n",
    "\n",
    "    loss = MSE (y_targets, q_values)\n",
    "\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743f455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function decorator to increase performance. Without this decorator our training will take twice as long\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(experiences, gamma, q_network, target_q_network)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    utils.update_target_network(q_network, target_q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv logger for DQN\n",
    "utils.init_logger(\"logs/dqn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46305806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train an evaluate DQN\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 5000\n",
    "max_num_timesteps = 200\n",
    "td_loss_total = 0\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "    # print(state)\n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        #to get max_q to plotting\n",
    "        max_q = max(q_values.numpy()[0])\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, terminated_vals, _ , _= env.step(action)\n",
    "        # Penalize distance from center\n",
    "        reward = 1.0 - abs(next_state[0])  \n",
    "        if abs(next_state[0]) > 0.5:\n",
    "            reward = 0\n",
    "        # print(reward)\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the terminated_vals variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, terminated_vals))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "                    \n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "            \n",
    "            agent_learn(experiences, GAMMA)\n",
    "\n",
    "        \n",
    "\n",
    "            \n",
    "        # print(type(next_state))\n",
    "        state = next_state.copy()\n",
    "        \n",
    "        total_points += reward\n",
    "        \n",
    "\n",
    "        if terminated_vals:\n",
    "            break\n",
    "             \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    #to get avg_reward for plotting\n",
    "    avg_reward = av_latest_points\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = utils.get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    #to log in csv file for later analysis\n",
    "    utils.log_metrics(\"logs/dqn.csv\", i, reward, avg_reward, td_loss_total, max_q)\n",
    "    \n",
    "    \n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 180 points in the last 100 episodes.\n",
    "    if av_latest_points >= 190.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('models/cart_pole_DQN_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab3677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv logger for DQN\n",
    "utils.init_logger(\"logs/ddqn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train an evaluate DDQN\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 5000\n",
    "max_num_timesteps = 200\n",
    "\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "td_loss_total = 0\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "    # print(state)\n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        #to get max_q to plotting\n",
    "        max_q = max(q_values.numpy()[0])\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, terminated_vals, _ , _= env.step(action)\n",
    "        # Penalize distance from center\n",
    "        reward = 1.0 - abs(next_state[0])  \n",
    "        if abs(next_state[0]) > 0.5:\n",
    "            reward = 0\n",
    "        # print(reward)\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the terminated_vals variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, terminated_vals))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "            # Sample random mini-batch of experience tuples (S,A,R,S') from D\n",
    "            experiences = utils.get_experiences(memory_buffer)\n",
    "\n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "\n",
    "            \n",
    "            agent_learn(experiences, GAMMA)\n",
    "            \n",
    "        # print(type(next_state))\n",
    "        state = next_state.copy()\n",
    "        \n",
    "        total_points += reward\n",
    "        \n",
    "\n",
    "        if terminated_vals:\n",
    "\n",
    "            break     \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    #to get avg_reward for plotting\n",
    "    avg_reward = av_latest_points\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = utils.get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    #to log in csv file for later analysis\n",
    "    \n",
    "    utils.log_metrics(\"logs/ddqn.csv\", i, reward, avg_reward, max_q)\n",
    "    \n",
    "    \n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >=  190.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('models/cart_pole_DDQN_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb86f99d",
   "metadata": {},
   "source": [
    "### To Train DDQN + PER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb3234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a csv logger for DQN + PER\n",
    "utils.init_logger(\"logs/ddqn_per.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ee11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store experiences as named tuples\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\", \"priority\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(experiences, gamma, q_network, target_q_network, weights):\n",
    "    \"\"\" \n",
    "    Calculates the loss.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "      q_network: (tf.keras.Sequential) Keras model for predicting the q_values\n",
    "      target_q_network: (tf.keras.Sequential) Keras model for predicting the targets\n",
    "          \n",
    "    Returns:\n",
    "      loss: (TensorFlow Tensor(shape=(0,), dtype=int32)) the Mean-Squared Error between\n",
    "            the y targets and the Q(s,a) values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack the mini-batch of experience tuples\n",
    "    states, actions, rewards, next_states, terminated_vals =  experiences\n",
    "    \n",
    "    # Convert weights to tf.float32\n",
    "    weights = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
    "\n",
    "    ### For Double DQN ###\n",
    "    # Q-values from target network for next_states\n",
    "    q_values_target = target_q_network(next_states)  # shape [batch_size, num_actions]\n",
    "    q_values = q_network(next_states)\n",
    "    # Greedy actions chosen by q_network\n",
    "    next_actions = tf.argmax(q_values, axis=1,output_type=tf.int32)  # shape [batch_size]\n",
    "    # Gather the Q-value for each chosen action\n",
    "    batch_indices = tf.range(tf.shape(next_actions)[0], dtype=tf.int32)\n",
    "    indices = tf.stack([batch_indices, next_actions], axis=1)\n",
    "    q_next_target = tf.gather_nd(q_values_target, indices)  # shape [batch_size]\n",
    "    y_targets = rewards + (1-terminated_vals)*(gamma*q_next_target)\n",
    "    \n",
    "\n",
    "    # Set y = R if episode terminates, otherwise set y = R + γ max Q^(s,a). \n",
    "\n",
    "\n",
    "    # Get the q_values and reshape to match y_targets\n",
    "    q_values = q_network(states)\n",
    "    q_values = tf.gather_nd(q_values, tf.stack([tf.range(q_values.shape[0]),\n",
    "                                                tf.cast(actions, tf.int32)], axis=1))\n",
    "        \n",
    "    # Compute the td_loss\n",
    "\n",
    "    td_errors = y_targets - q_values \n",
    "    # --- Compute PER-weighted loss ---\n",
    "    loss = tf.reduce_mean(tf.square(td_errors) * weights)\n",
    "    \n",
    "    return loss, td_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e75100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function decorator to increase performance. Without this decorator our training will take twice as long\n",
    "@tf.function\n",
    "def agent_learn(experiences, gamma, weights):\n",
    "    \"\"\"\n",
    "    Updates the weights of the Q networks.\n",
    "    \n",
    "    Args:\n",
    "      experiences: (tuple) tuple of [\"state\", \"action\", \"reward\", \"next_state\", \"terminated_vals\"] namedtuples\n",
    "      gamma: (float) The discount factor.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, td_errors = compute_loss(experiences, gamma, q_network, target_q_network, weights)\n",
    "\n",
    "    # Get the gradients of the loss with respect to the weights.\n",
    "    gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "    \n",
    "    # Update the weights of the q_network.\n",
    "    optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "    # update the weights of target q_network\n",
    "    utils.update_target_network(q_network, target_q_network)\n",
    "    \n",
    "    # Return TD-errors to update PER priorities\n",
    "    return td_errors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train an evaluate DDQN + PER\n",
    "start = time.time()\n",
    "\n",
    "num_episodes = 5000\n",
    "max_num_timesteps = 200\n",
    "steps = 0\n",
    "total_point_history = []\n",
    "\n",
    "num_p_av = 100    # number of total points to use for averaging\n",
    "epsilon = 1.0     # initial ε value for ε-greedy policy\n",
    "\n",
    "# Create a memory buffer D with capacity N\n",
    "memory_buffer = deque(maxlen=MEMORY_SIZE)\n",
    "priorities = np.ones(MEMORY_SIZE)  # same length as buffer\n",
    "td_loss_total = 0\n",
    "# Set the target network weights equal to the Q-Network weights\n",
    "target_q_network.set_weights(q_network.get_weights())\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    # Reset the environment to the initial state and get the initial state\n",
    "    state, _ = env.reset()\n",
    "    total_points = 0\n",
    "    # print(state)\n",
    "    for t in range(max_num_timesteps):\n",
    "        \n",
    "        # From the current state S choose an action A using an ε-greedy policy\n",
    "        state_qn = np.expand_dims(state, axis=0)  # state needs to be the right shape for the q_network\n",
    "        q_values = q_network(state_qn)\n",
    "        action = utils.get_action(q_values, epsilon)\n",
    "        \n",
    "        #to get max_q to plotting\n",
    "        max_q = max(q_values.numpy()[0])\n",
    "        \n",
    "        # Take action A and receive reward R and the next state S'\n",
    "        next_state, reward, terminated_vals, _ , _= env.step(action)\n",
    "        # Penalize distance from center\n",
    "        reward = 1.0 - abs(next_state[0])  \n",
    "        if abs(next_state[0]) > 0.5:\n",
    "            reward = 0\n",
    "        # print(reward)\n",
    "        # Store experience tuple (S,A,R,S') in the memory buffer.\n",
    "        # We store the terminated_vals variable as well for convenience.\n",
    "        memory_buffer.append(experience(state, action, reward, next_state, terminated_vals, priority=1.0))\n",
    "        \n",
    "        # Only update the network every NUM_STEPS_FOR_UPDATE time steps.\n",
    "        update = utils.check_update_conditions(t, NUM_STEPS_FOR_UPDATE, memory_buffer)\n",
    "        \n",
    "        if update:\n",
    "           # Sample experiences from PER buffer\n",
    "            steps += 1\n",
    "            experiences, indices, weights = utils.get_experiences_PER(memory_buffer, step=steps)\n",
    "            # Set the y targets, perform a gradient descent step,\n",
    "            # and update the network weights.\n",
    "\n",
    "            \n",
    "            td_errors =  agent_learn(experiences, GAMMA, weights)\n",
    "            # Update priorities in memory\n",
    "            new_priorities = np.abs(td_errors.numpy()) + 1e-6\n",
    "            # Directly update the priorities array\n",
    "            # Update the priority in the experience itself\n",
    "            for idx, p in zip(indices, new_priorities):\n",
    "                old_exp = memory_buffer[idx]\n",
    "                memory_buffer[idx] = experience(\n",
    "                    state=old_exp.state,\n",
    "                    action=old_exp.action,\n",
    "                    reward=old_exp.reward,\n",
    "                    next_state=old_exp.next_state,\n",
    "                    terminated_vals=old_exp.terminated_vals,\n",
    "                    priority=float(p)\n",
    "                )\n",
    "            \n",
    "        # print(type(next_state))\n",
    "        state = next_state.copy()\n",
    "        \n",
    "        total_points += reward\n",
    "        \n",
    "\n",
    "        if terminated_vals:\n",
    "\n",
    "            break     \n",
    "    total_point_history.append(total_points)\n",
    "    av_latest_points = np.mean(total_point_history[-num_p_av:])\n",
    "    \n",
    "    #to get avg_reward for plotting\n",
    "    avg_reward = av_latest_points\n",
    "    \n",
    "    # Update the ε value\n",
    "    epsilon = utils.get_new_eps(epsilon)\n",
    "\n",
    "    print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\", end=\"\")\n",
    "\n",
    "    if (i+1) % num_p_av == 0:\n",
    "        print(f\"\\rEpisode {i+1} | Total point average of the last {num_p_av} episodes: {av_latest_points:.2f}\")\n",
    "\n",
    "    #to log in csv file for later analysis\n",
    "    \n",
    "    utils.log_metrics(\"logs/ddqn_per.csv\", i, reward, avg_reward, max_q)\n",
    "    \n",
    "    \n",
    "    # We will consider that the environment is solved if we get an\n",
    "    # average of 200 points in the last 100 episodes.\n",
    "    if av_latest_points >=  190.0:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_network.save('models/cart_pole_DDQN_PER_model.h5')\n",
    "        break\n",
    "        \n",
    "tot_time = time.time() - start\n",
    "\n",
    "print(f\"\\nTotal Runtime: {tot_time:.2f} s ({(tot_time/60):.2f} min)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef1add8",
   "metadata": {},
   "source": [
    "# Exporting Video of Untrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a98e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12ca85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "# env = gym.make(\"CartPole-v0\",render_mode=\"human\")  # or the env you trained on\n",
    "env = gym.make(\"CartPole-v0\",render_mode=\"rgb_array\")  # or the env you trained on\n",
    "\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True,name_prefix = \"untrained_cart_pole\")\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "start_time = time.time()\n",
    "while not done and ((time.time() - start_time)) <= 10: # stop the render if done or 30 sec whatever it first\n",
    "    env.render()  # optional, shows the agent in action\n",
    "    action = env.action_space.sample() # to show without training how it works\n",
    "    # Step in environment\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    if terminated == True:\n",
    "        env.reset()\n",
    "    state = next_state\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from moviepy import concatenate_videoclips, VideoFileClip\n",
    "\n",
    "video_dir = \"videos\"\n",
    "prefix = \"untrained_cart_pole\"\n",
    "\n",
    "# Collect files matching prefix\n",
    "video_files = sorted([f for f in os.listdir(video_dir) if f.startswith(prefix) and f.endswith(\".mp4\")])\n",
    "\n",
    "# Load clips\n",
    "clips = [VideoFileClip(os.path.join(video_dir, f)) for f in video_files]\n",
    "\n",
    "# Concatenate into one video\n",
    "final = concatenate_videoclips(clips)\n",
    "output_path = os.path.join(video_dir, f\"{prefix}_combined.mp4\")\n",
    "final.write_videofile(output_path)\n",
    "\n",
    "# Close clips\n",
    "for clip in clips:\n",
    "    clip.close()\n",
    "\n",
    "# Delete originals\n",
    "for f in video_files:\n",
    "    os.remove(os.path.join(video_dir, f))\n",
    "\n",
    "print(f\"✅ Combined video saved at {output_path} and original episode videos deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be382077",
   "metadata": {},
   "source": [
    "# Exporting Video of Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model\n",
    "model = load_model('models/cart_pole_DDQN_PER_model.h5')\n",
    "# Create environment\n",
    "# env = gym.make(\"CartPole-v0\",render_mode=\"human\")  # or the env you trained on\n",
    "env = gym.make(\"CartPole-v0\",render_mode=\"rgb_array\")  # or the env you trained on\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"videos\", episode_trigger=lambda e: True,name_prefix = \"DDQN_PER_trained_cart_pole\", video_length= 1500)\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "for _ in range(1500):  # run exactly video_length steps\n",
    "    # env.render()  # optional, shows the agent in action\n",
    "\n",
    "    # Prepare state for network\n",
    "    state_input = np.expand_dims(state, axis=0)\n",
    "\n",
    "    # Get Q-values and pick greedy action\n",
    "    q_values = model(state_input)\n",
    "    action = np.argmax(q_values.numpy()[0])\n",
    "\n",
    "    # Step in environment\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated \n",
    "    if done == True:\n",
    "        break\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"Episode finished. Total reward:\", total_reward)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58123beb",
   "metadata": {},
   "source": [
    "# To Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd15c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_comparison(\"figures/comparison.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b0e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690aef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eaf9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba1dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c936dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
